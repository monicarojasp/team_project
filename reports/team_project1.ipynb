{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene expression profiles to identify cancer types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "#feature selection\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from github?\n",
    "# Define the base directory for the data files\n",
    "# Define the raw URLs for the data files\n",
    "base_dir = '../data/raw/'\n",
    "\n",
    "\n",
    "# Construct the full paths\n",
    "data_path = os.path.join(base_dir, 'data.csv')\n",
    "labels_path = os.path.join(base_dir, 'labels.csv')\n",
    "\n",
    "dt = pd.read_csv(data_path, index_col = 0)\n",
    "lb = pd.read_csv(labels_path, index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the data with labels\n",
    "\n",
    "data_frame = pd.merge(dt, lb, left_index=True, right_index=True)\n",
    "print(data_frame.columns)\n",
    "print (data_frame.shape)\n",
    "\n",
    "#Rename last column as Cancer_Type\n",
    "data_frame = data_frame.rename(columns={'Class': 'Cancer_Type'})\n",
    "print(data_frame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values - this summs the amount of null occurrences in the data set and safe the columns with null values in variable null\n",
    "datanull = data_frame.isnull().sum() \n",
    "null = [i for i in datanull if i > 0]\n",
    "\n",
    "print('The columns with missing values are:%d'%len(null))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking how many type of cancer we have\n",
    "print(data_frame['Cancer_Type'].value_counts())\n",
    "\n",
    "#Visualizing\n",
    "data_frame['Cancer_Type'].value_counts().plot.bar(color='purple')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Cancer Types')\n",
    "plt.ylabel('# samples per type')\n",
    "plt.title('Amount of samples per cancer type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating feature values from class (cancer type)\n",
    "X = data_frame.iloc[:, :-1]  # Features (gene expression levels)\n",
    "y = data_frame.iloc[:, -1]   # Target variable (last column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the lables \n",
    "Since the class (cancer type) is categorical we will need to convert (encode) them to numeric to be able to perform the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using LabelEncoder from sklearn \n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y_encoded = label_encoder.transform(y)\n",
    "labels = label_encoder.classes_\n",
    "classes = np.unique(y_encoded)\n",
    "print(labels)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data spliting\n",
    "\n",
    "This section will allow us to split the data into training and testing subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y_encoded,test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization\n",
    "\n",
    "This will imporve the model performace, it will set the values to the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_norm = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test_norm = min_max_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Selecting the top genes(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic_features = mutual_info_classif(X_train_norm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarily selected top #\n",
    "#you can modify the value and see how the performance of the model changes\n",
    "\n",
    "n_features=50\n",
    "selected_scores_indices=np.argsort(mic_features)[::-1][0:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected=X_train_norm[:,selected_scores_indices]\n",
    "X_test_selected=X_test_norm[:,selected_scores_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "For the purpose of this project we are doing a regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Visualization of the results\n",
    " \n",
    " #### Scatter Plot of Actual vs. Predicted Values\n",
    " Points clustered closely around the diagonal line indicate accurate predictions. Deviations from this line suggest where the model overpredicts or underpredicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of actual vs. predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Diagonal line\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Plot\n",
    "\n",
    "The residual plot shows the residuals (difference between actual and predicted values) against the predicted values.\n",
    "\n",
    "Ideally, residuals should be randomly scattered around the horizontal line at y=0, without any clear pattern. Patterns could indicate heteroscedasticity or other issues with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred, y_test - y_pred, color='blue', alpha=0.5)\n",
    "plt.axhline(y=0, color='k', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of Residuals\n",
    "Visualizing the distribution of residuals can provide insights into the model's performance. Residuals ideally should be normally distributed around 0. Skewness or heavy tails indicate bias in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot of residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "residuals = y_test - y_pred\n",
    "plt.hist(residuals, bins=20, color='blue', alpha=0.5)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient Plot\n",
    "\n",
    "Show which features are most influential in predicting the target variable. Larger coefficients (positive or negative) indicate stronger influence of the corresponding feature on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selecteddf = pd.DataFrame(X_train_selected)\n",
    "\n",
    "# Coefficient plot (if applicable)\n",
    "if hasattr(model, 'coef_'):\n",
    "    coefs = pd.Series(model.coef_, index=X_train_selecteddf.columns)\n",
    "    coefs.plot(kind='bar')\n",
    "    plt.title('Feature Coefficients')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize logistic regression model\n",
    "model_lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model_lr.fit(X_train_selected, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_lr = model_lr.predict(X_test_selected)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "Here, we are testing a different way to reduce dimensionality by using Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/raw/data.csv\")\n",
    "labels = pd.read_csv(\"../data/raw/labels.csv\")\n",
    "data.head()\n",
    "\n",
    "X = data.drop(data.columns[0],axis=1)\n",
    "Y = labels.Class\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "principalComponents = pca.fit_transform(X)\n",
    "X_pca = pd.DataFrame(data = principalComponents)\n",
    "X_pca = pd.concat([X_pca.reset_index().drop(['index'],axis=1),Y.reset_index().drop(['index'],axis=1)], axis=1)\n",
    "\n",
    "sns.pairplot(x_vars=0, y_vars=1, data=X_pca, hue=\"Class\",palette=\"YlGnBu\",size=7,aspect=1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_pca.drop(['Class'],axis=1)\n",
    "Y = X_pca['Class']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y_encoded, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_pca = model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred_pca)\n",
    "r2 = r2_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred_pca\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred_pca, residuals, color='blue', alpha=0.5)\n",
    "plt.axhline(y=0, color='k', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot of residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "residuals = y_test - y_pred_pca\n",
    "plt.hist(residuals, bins=20, color='blue', alpha=0.5)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
